{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Dataset\n",
    "\n",
    "## What happens when you smash things together near the speed of light?\n",
    "\n",
    "The Higgs boson has been sought after for decades. Can we use machine learning to gather any more information about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "callback = keras.callbacks.TensorBoard(log_dir='./logs',histogram_freq=0, batch_size=32, write_graph=True)\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = ['Class Label', 'lepton pT', 'lepton eta', \n",
    "                'lepton phi', 'missing energy magnitude',\n",
    "                'missing energy phi', 'jet 1 pt', 'jet 1 eta',\n",
    "                'jet 1 phi', 'jet 1 b-tag', 'jet 2 pt',  'jet 2 eta', \n",
    "                'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt', 'jet 3 eta', \n",
    "                'jet 3 phi',' jet 3 b-tag', 'jet 4 pt', 'jet 4 eta', \n",
    "                'jet 4 phi', 'jet 4 b-tag', 'm_jj', 'm_jjj', 'm_lv', \n",
    "                'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/HIGGS.csv', names = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Label</th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Label  lepton pT  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "0          1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1          1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2          1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3          0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4          1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag    ...     \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064     0.000000    ...      \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230     2.173076    ...      \n",
       "2            0.425629  1.104875   1.282322   1.381664     0.000000    ...      \n",
       "3            0.882454  1.786066  -1.646778  -0.942383     0.000000    ...      \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871     0.000000    ...      \n",
       "\n",
       "   jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0  -0.010455  -0.045767     3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1  -1.138930  -0.000819     0.000000  0.302220  0.833048  0.985700  0.978098   \n",
       "2   1.128848   0.900461     0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "3  -0.678379  -1.360356     0.000000  0.946652  1.028704  0.998656  0.728281   \n",
       "4  -0.373566   0.113041     0.000000  0.755856  1.361057  0.986610  0.838085   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "0  0.721657  0.988751  0.876678  \n",
       "1  0.779732  0.992356  0.798343  \n",
       "2  0.803252  0.865924  0.780118  \n",
       "3  0.869200  1.026736  0.957904  \n",
       "4  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Label</th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.299203e-01</td>\n",
       "      <td>9.914658e-01</td>\n",
       "      <td>-8.297618e-06</td>\n",
       "      <td>-1.327225e-05</td>\n",
       "      <td>9.985364e-01</td>\n",
       "      <td>2.613459e-05</td>\n",
       "      <td>9.909152e-01</td>\n",
       "      <td>-2.027520e-05</td>\n",
       "      <td>7.716199e-06</td>\n",
       "      <td>9.999687e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.756954e-06</td>\n",
       "      <td>1.744903e-05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.034290e+00</td>\n",
       "      <td>1.024805e+00</td>\n",
       "      <td>1.050554e+00</td>\n",
       "      <td>1.009742e+00</td>\n",
       "      <td>9.729596e-01</td>\n",
       "      <td>1.033036e+00</td>\n",
       "      <td>9.598120e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.991040e-01</td>\n",
       "      <td>5.653777e-01</td>\n",
       "      <td>1.008827e+00</td>\n",
       "      <td>1.006346e+00</td>\n",
       "      <td>6.000185e-01</td>\n",
       "      <td>1.006326e+00</td>\n",
       "      <td>4.749747e-01</td>\n",
       "      <td>1.009303e+00</td>\n",
       "      <td>1.005901e+00</td>\n",
       "      <td>1.027808e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007694e+00</td>\n",
       "      <td>1.006366e+00</td>\n",
       "      <td>1.400209e+00</td>\n",
       "      <td>6.746354e-01</td>\n",
       "      <td>3.808074e-01</td>\n",
       "      <td>1.645763e-01</td>\n",
       "      <td>3.974453e-01</td>\n",
       "      <td>5.254063e-01</td>\n",
       "      <td>3.652556e-01</td>\n",
       "      <td>3.133378e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.746966e-01</td>\n",
       "      <td>-2.434976e+00</td>\n",
       "      <td>-1.742508e+00</td>\n",
       "      <td>2.370088e-04</td>\n",
       "      <td>-1.743944e+00</td>\n",
       "      <td>1.375024e-01</td>\n",
       "      <td>-2.969725e+00</td>\n",
       "      <td>-1.741237e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.497265e+00</td>\n",
       "      <td>-1.742691e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.507046e-02</td>\n",
       "      <td>1.986757e-01</td>\n",
       "      <td>8.304866e-02</td>\n",
       "      <td>1.320062e-01</td>\n",
       "      <td>4.786215e-02</td>\n",
       "      <td>2.951122e-01</td>\n",
       "      <td>3.307214e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.907533e-01</td>\n",
       "      <td>-7.383225e-01</td>\n",
       "      <td>-8.719308e-01</td>\n",
       "      <td>5.768156e-01</td>\n",
       "      <td>-8.712081e-01</td>\n",
       "      <td>6.789927e-01</td>\n",
       "      <td>-6.872450e-01</td>\n",
       "      <td>-8.680962e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.141902e-01</td>\n",
       "      <td>-8.714789e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.906095e-01</td>\n",
       "      <td>8.462266e-01</td>\n",
       "      <td>9.857525e-01</td>\n",
       "      <td>7.675732e-01</td>\n",
       "      <td>6.738168e-01</td>\n",
       "      <td>8.193964e-01</td>\n",
       "      <td>7.703901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.533714e-01</td>\n",
       "      <td>-5.415563e-05</td>\n",
       "      <td>-2.410638e-04</td>\n",
       "      <td>8.916277e-01</td>\n",
       "      <td>2.125454e-04</td>\n",
       "      <td>8.948193e-01</td>\n",
       "      <td>-2.543566e-05</td>\n",
       "      <td>5.813991e-05</td>\n",
       "      <td>1.086538e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.721330e-04</td>\n",
       "      <td>-2.642369e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.949304e-01</td>\n",
       "      <td>9.506853e-01</td>\n",
       "      <td>9.897798e-01</td>\n",
       "      <td>9.165110e-01</td>\n",
       "      <td>8.733798e-01</td>\n",
       "      <td>9.473447e-01</td>\n",
       "      <td>8.719701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.236226e+00</td>\n",
       "      <td>7.382142e-01</td>\n",
       "      <td>8.709940e-01</td>\n",
       "      <td>1.293056e+00</td>\n",
       "      <td>8.714708e-01</td>\n",
       "      <td>1.170740e+00</td>\n",
       "      <td>6.871941e-01</td>\n",
       "      <td>8.683126e-01</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.141017e-01</td>\n",
       "      <td>8.716055e-01</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>1.024730e+00</td>\n",
       "      <td>1.083493e+00</td>\n",
       "      <td>1.020528e+00</td>\n",
       "      <td>1.142226e+00</td>\n",
       "      <td>1.138439e+00</td>\n",
       "      <td>1.140458e+00</td>\n",
       "      <td>1.059248e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.209891e+01</td>\n",
       "      <td>2.434868e+00</td>\n",
       "      <td>1.743236e+00</td>\n",
       "      <td>1.539682e+01</td>\n",
       "      <td>1.743257e+00</td>\n",
       "      <td>9.940391e+00</td>\n",
       "      <td>2.969674e+00</td>\n",
       "      <td>1.741454e+00</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.498009e+00</td>\n",
       "      <td>1.743372e+00</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>4.019237e+01</td>\n",
       "      <td>2.037278e+01</td>\n",
       "      <td>7.992739e+00</td>\n",
       "      <td>1.426244e+01</td>\n",
       "      <td>1.776285e+01</td>\n",
       "      <td>1.149652e+01</td>\n",
       "      <td>8.374498e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Label     lepton pT    lepton eta    lepton phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   5.299203e-01  9.914658e-01 -8.297618e-06 -1.327225e-05   \n",
       "std    4.991040e-01  5.653777e-01  1.008827e+00  1.006346e+00   \n",
       "min    0.000000e+00  2.746966e-01 -2.434976e+00 -1.742508e+00   \n",
       "25%    0.000000e+00  5.907533e-01 -7.383225e-01 -8.719308e-01   \n",
       "50%    1.000000e+00  8.533714e-01 -5.415563e-05 -2.410638e-04   \n",
       "75%    1.000000e+00  1.236226e+00  7.382142e-01  8.709940e-01   \n",
       "max    1.000000e+00  1.209891e+01  2.434868e+00  1.743236e+00   \n",
       "\n",
       "       missing energy magnitude  missing energy phi      jet 1 pt  \\\n",
       "count              1.100000e+07        1.100000e+07  1.100000e+07   \n",
       "mean               9.985364e-01        2.613459e-05  9.909152e-01   \n",
       "std                6.000185e-01        1.006326e+00  4.749747e-01   \n",
       "min                2.370088e-04       -1.743944e+00  1.375024e-01   \n",
       "25%                5.768156e-01       -8.712081e-01  6.789927e-01   \n",
       "50%                8.916277e-01        2.125454e-04  8.948193e-01   \n",
       "75%                1.293056e+00        8.714708e-01  1.170740e+00   \n",
       "max                1.539682e+01        1.743257e+00  9.940391e+00   \n",
       "\n",
       "          jet 1 eta     jet 1 phi   jet 1 b-tag      ...          jet 4 eta  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07      ...       1.100000e+07   \n",
       "mean  -2.027520e-05  7.716199e-06  9.999687e-01      ...      -5.756954e-06   \n",
       "std    1.009303e+00  1.005901e+00  1.027808e+00      ...       1.007694e+00   \n",
       "min   -2.969725e+00 -1.741237e+00  0.000000e+00      ...      -2.497265e+00   \n",
       "25%   -6.872450e-01 -8.680962e-01  0.000000e+00      ...      -7.141902e-01   \n",
       "50%   -2.543566e-05  5.813991e-05  1.086538e+00      ...       3.721330e-04   \n",
       "75%    6.871941e-01  8.683126e-01  2.173076e+00      ...       7.141017e-01   \n",
       "max    2.969674e+00  1.741454e+00  2.173076e+00      ...       2.498009e+00   \n",
       "\n",
       "          jet 4 phi   jet 4 b-tag          m_jj         m_jjj          m_lv  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.744903e-05  1.000000e+00  1.034290e+00  1.024805e+00  1.050554e+00   \n",
       "std    1.006366e+00  1.400209e+00  6.746354e-01  3.808074e-01  1.645763e-01   \n",
       "min   -1.742691e+00  0.000000e+00  7.507046e-02  1.986757e-01  8.304866e-02   \n",
       "25%   -8.714789e-01  0.000000e+00  7.906095e-01  8.462266e-01  9.857525e-01   \n",
       "50%   -2.642369e-04  0.000000e+00  8.949304e-01  9.506853e-01  9.897798e-01   \n",
       "75%    8.716055e-01  3.101961e+00  1.024730e+00  1.083493e+00  1.020528e+00   \n",
       "max    1.743372e+00  3.101961e+00  4.019237e+01  2.037278e+01  7.992739e+00   \n",
       "\n",
       "              m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  \n",
       "mean   1.009742e+00  9.729596e-01  1.033036e+00  9.598120e-01  \n",
       "std    3.974453e-01  5.254063e-01  3.652556e-01  3.133378e-01  \n",
       "min    1.320062e-01  4.786215e-02  2.951122e-01  3.307214e-01  \n",
       "25%    7.675732e-01  6.738168e-01  8.193964e-01  7.703901e-01  \n",
       "50%    9.165110e-01  8.733798e-01  9.473447e-01  8.719701e-01  \n",
       "75%    1.142226e+00  1.138439e+00  1.140458e+00  1.059248e+00  \n",
       "max    1.426244e+01  1.776285e+01  1.149652e+01  8.374498e+00  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.374992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.812581</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.423265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  \\\n",
       "0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1   0.907542    0.329147    0.359412                  1.497970   \n",
       "2   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064     0.000000  1.374992   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230     2.173076  0.812581   \n",
       "2            0.425629  1.104875   1.282322   1.381664     0.000000  0.851737   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383     0.000000  2.423265   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871     0.000000  0.800872   \n",
       "\n",
       "     ...     jet 4 eta  jet 4 phi  jet 4 b-tag      m_jj     m_jjj      m_lv  \\\n",
       "0    ...     -0.010455  -0.045767     3.101961  1.353760  0.979563  0.978076   \n",
       "1    ...     -1.138930  -0.000819     0.000000  0.302220  0.833048  0.985700   \n",
       "2    ...      1.128848   0.900461     0.000000  0.909753  1.108330  0.985692   \n",
       "3    ...     -0.678379  -1.360356     0.000000  0.946652  1.028704  0.998656   \n",
       "4    ...     -0.373566   0.113041     0.000000  0.755856  1.361057  0.986610   \n",
       "\n",
       "      m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0  0.920005  0.721657  0.988751  0.876678  \n",
       "1  0.978098  0.779732  0.992356  0.798343  \n",
       "2  0.951331  0.803252  0.865924  0.780118  \n",
       "3  0.728281  0.869200  1.026736  0.957904  \n",
       "4  0.838085  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"Class Label\"]\n",
    "labels.head()\n",
    "\n",
    "df.drop([\"Class Label\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try using decision tree\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_X_vals = scaler.transform(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X_vals, labels, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6783970129475747\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "fitted = clf.fit(X_train, y_train, callbacks=)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"f1 score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn import tree\n",
    "# import graphviz\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "#                          feature_names=df.keys(),  \n",
    "#                          class_names=[\"No Higgs\",\"Higgs\"],  \n",
    "#                          filled=True, rounded=True,  \n",
    "#                          special_characters=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph\n",
    "#Takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500000, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result = []\n",
    "accuracy_result = []\n",
    "label_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                464       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,349\n",
      "Trainable params: 1,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "seq_model = Sequential()\n",
    "seq_model.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "seq_model.add(Dense(16, activation='sigmoid'))\n",
    "seq_model.add(Dense(4, activation='sigmoid'))\n",
    "seq_model.add(Dense(1, activation='sigmoid'))\n",
    "seq_model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/10\n",
      "5500000/5500000 [==============================] - 174s 32us/step - loss: 0.6638 - acc: 0.5927 - val_loss: 0.6338 - val_acc: 0.6409\n",
      "Epoch 2/10\n",
      "5500000/5500000 [==============================] - 151s 28us/step - loss: 0.6255 - acc: 0.6552 - val_loss: 0.6171 - val_acc: 0.6653\n",
      "Epoch 3/10\n",
      "5500000/5500000 [==============================] - 146s 27us/step - loss: 0.6027 - acc: 0.6777 - val_loss: 0.5869 - val_acc: 0.6909\n",
      "Epoch 4/10\n",
      "5500000/5500000 [==============================] - 155s 28us/step - loss: 0.5770 - acc: 0.6986 - val_loss: 0.5691 - val_acc: 0.7055\n",
      "Epoch 5/10\n",
      "5500000/5500000 [==============================] - 154s 28us/step - loss: 0.5605 - acc: 0.7105 - val_loss: 0.5543 - val_acc: 0.7151\n",
      "Epoch 6/10\n",
      "5500000/5500000 [==============================] - 154s 28us/step - loss: 0.5507 - acc: 0.7171 - val_loss: 0.5467 - val_acc: 0.7193\n",
      "Epoch 7/10\n",
      "5500000/5500000 [==============================] - 150s 27us/step - loss: 0.5453 - acc: 0.7204 - val_loss: 0.5433 - val_acc: 0.7218\n",
      "Epoch 8/10\n",
      "5500000/5500000 [==============================] - 141s 26us/step - loss: 0.5416 - acc: 0.7229 - val_loss: 0.5396 - val_acc: 0.7240\n",
      "Epoch 9/10\n",
      "5500000/5500000 [==============================] - 148s 27us/step - loss: 0.5387 - acc: 0.7248 - val_loss: 0.5366 - val_acc: 0.7262\n",
      "Epoch 10/10\n",
      "5500000/5500000 [==============================] - 187s 34us/step - loss: 0.5361 - acc: 0.7266 - val_loss: 0.5344 - val_acc: 0.7277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1988f748>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.fit(X_train, y_train, \n",
    "              batch_size=64, epochs=10, \n",
    "              verbose=1, validation_data=(X_test, y_test),\n",
    "              callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".550\")\n",
    "accuracy_result.append(\".71\")\n",
    "label_result.append(\"4LySigBinCrosSGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                464       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,349\n",
      "Trainable params: 1,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tanh_model = Sequential()\n",
    "tanh_model.add(Dense(28, activation='tanh', input_shape=(28,)))\n",
    "tanh_model.add(Dense(16, activation='tanh'))\n",
    "tanh_model.add(Dense(4, activation='tanh'))\n",
    "tanh_model.add(Dense(1, activation='tanh'))\n",
    "tanh_model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "tanh_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/10\n",
      "5500000/5500000 [==============================] - 156s 28us/step - loss: 0.5979 - acc: 0.6792 - val_loss: 0.5634 - val_acc: 0.7088\n",
      "Epoch 2/10\n",
      "5500000/5500000 [==============================] - 216s 39us/step - loss: 0.5598 - acc: 0.7115 - val_loss: 0.5496 - val_acc: 0.7173\n",
      "Epoch 3/10\n",
      "5500000/5500000 [==============================] - 226s 41us/step - loss: 0.5980 - acc: 0.7123 - val_loss: 0.5396 - val_acc: 0.7234\n",
      "Epoch 4/10\n",
      "5500000/5500000 [==============================] - 150s 27us/step - loss: 0.7252 - acc: 0.6835 - val_loss: 0.5662 - val_acc: 0.7042\n",
      "Epoch 5/10\n",
      "5500000/5500000 [==============================] - 136s 25us/step - loss: 0.5559 - acc: 0.7166 - val_loss: 0.5423 - val_acc: 0.7227\n",
      "Epoch 6/10\n",
      "5500000/5500000 [==============================] - 135s 25us/step - loss: 0.5707 - acc: 0.7230 - val_loss: 0.5371 - val_acc: 0.7261\n",
      "Epoch 7/10\n",
      "5500000/5500000 [==============================] - 143s 26us/step - loss: 3.6289 - acc: 0.6393 - val_loss: 7.4909 - val_acc: 0.5301\n",
      "Epoch 8/10\n",
      "5500000/5500000 [==============================] - 143s 26us/step - loss: 7.4975 - acc: 0.5297 - val_loss: 7.4909 - val_acc: 0.5301\n",
      "Epoch 9/10\n",
      "5500000/5500000 [==============================] - 135s 25us/step - loss: 7.4975 - acc: 0.5297 - val_loss: 7.4909 - val_acc: 0.5301\n",
      "Epoch 10/10\n",
      "5500000/5500000 [==============================] - 135s 25us/step - loss: 7.4975 - acc: 0.5297 - val_loss: 7.4909 - val_acc: 0.5301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2372b080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".71\")\n",
    "accuracy_result.append(\".52\")\n",
    "label_result.append(\"3LyTanhBinCrosSGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                290       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "two_lay = Sequential()\n",
    "two_lay.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "two_lay.add(Dense(10, activation='sigmoid'))\n",
    "two_lay.add(Dense(1, activation='sigmoid'))\n",
    "two_lay.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "two_lay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/5\n",
      "5500000/5500000 [==============================] - 131s 24us/step - loss: 0.6472 - acc: 0.6181 - val_loss: 0.6269 - val_acc: 0.6490\n",
      "Epoch 2/5\n",
      "5500000/5500000 [==============================] - 131s 24us/step - loss: 0.6111 - acc: 0.6653 - val_loss: 0.5930 - val_acc: 0.6837\n",
      "Epoch 3/5\n",
      "5500000/5500000 [==============================] - 146s 27us/step - loss: 0.5813 - acc: 0.6934 - val_loss: 0.5720 - val_acc: 0.7005\n",
      "Epoch 4/5\n",
      "5500000/5500000 [==============================] - 191s 35us/step - loss: 0.5668 - acc: 0.7044 - val_loss: 0.5616 - val_acc: 0.7077\n",
      "Epoch 5/5\n",
      "5500000/5500000 [==============================] - 162s 30us/step - loss: 0.5583 - acc: 0.7111 - val_loss: 0.5546 - val_acc: 0.7144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9aa0ef60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_lay.fit(X_train, y_train, batch_size=64, epochs=5, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".55\")\n",
    "accuracy_result.append(\".71\")\n",
    "label_result.append(\"3LySigBinCrosSGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                290       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "two_lay = Sequential()\n",
    "two_lay.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "two_lay.add(Dense(10, activation='sigmoid'))\n",
    "two_lay.add(Dense(1, activation='sigmoid'))\n",
    "two_lay.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "two_lay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 162s 29us/step - loss: 0.5544 - acc: 0.7112 - val_loss: 0.5339 - val_acc: 0.7271\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 182s 33us/step - loss: 0.5283 - acc: 0.7311 - val_loss: 0.5237 - val_acc: 0.7341\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 163s 30us/step - loss: 0.5231 - acc: 0.7342 - val_loss: 0.5210 - val_acc: 0.7355\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 149s 27us/step - loss: 0.5207 - acc: 0.7358 - val_loss: 0.5193 - val_acc: 0.7367\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 191s 35us/step - loss: 0.5191 - acc: 0.7368 - val_loss: 0.5193 - val_acc: 0.7368\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 160s 29us/step - loss: 0.5181 - acc: 0.7375 - val_loss: 0.5170 - val_acc: 0.7379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9aa0e2e8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_lay.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".52\")\n",
    "accuracy_result.append(\".73\")\n",
    "label_result.append(\"3LySigBinCrosAdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                290       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rms_prop = Sequential()\n",
    "rms_prop.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "rms_prop.add(Dense(10, activation='sigmoid'))\n",
    "rms_prop.add(Dense(1, activation='sigmoid'))\n",
    "rms_prop.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "rms_prop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 156s 28us/step - loss: 0.5594 - acc: 0.7074 - val_loss: 0.5351 - val_acc: 0.7264\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 174s 32us/step - loss: 0.5299 - acc: 0.7299 - val_loss: 0.5253 - val_acc: 0.7331\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 151s 28us/step - loss: 0.5239 - acc: 0.7341 - val_loss: 0.5237 - val_acc: 0.7344\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 180s 33us/step - loss: 0.5216 - acc: 0.7358 - val_loss: 0.5246 - val_acc: 0.7332\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 202s 37us/step - loss: 0.5203 - acc: 0.7366 - val_loss: 0.5195 - val_acc: 0.7370\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 175s 32us/step - loss: 0.5193 - acc: 0.7373 - val_loss: 0.5190 - val_acc: 0.7376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a39e66400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_prop.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".51\")\n",
    "accuracy_result.append(\".73\")\n",
    "label_result.append(\"3LySigBinCrosRMSP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                290       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kld = Sequential()\n",
    "kld.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "kld.add(Dense(10, activation='sigmoid'))\n",
    "kld.add(Dense(1, activation='sigmoid'))\n",
    "kld.compile(loss='kullback_leibler_divergence', optimizer='rmsprop', metrics=['accuracy'])\n",
    "kld.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 151s 27us/step - loss: 0.5172 - acc: 0.7380 - val_loss: 0.5173 - val_acc: 0.7376\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 159s 29us/step - loss: 0.5166 - acc: 0.7383 - val_loss: 0.5166 - val_acc: 0.7382\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 166s 30us/step - loss: 0.5162 - acc: 0.7385 - val_loss: 0.5172 - val_acc: 0.7378\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 152s 28us/step - loss: 0.5158 - acc: 0.7388 - val_loss: 0.5149 - val_acc: 0.7396\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 176s 32us/step - loss: 0.5156 - acc: 0.7387 - val_loss: 0.5152 - val_acc: 0.7391\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 214s 39us/step - loss: 0.5155 - acc: 0.7391 - val_loss: 0.5144 - val_acc: 0.7398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9aa260b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_lay.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".51\")\n",
    "accuracy_result.append(\".74\")\n",
    "label_result.append(\"3LySigBinCrosAdam2nd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                290       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "poisson = Sequential()\n",
    "poisson.add(Dense(28, activation='sigmoid', input_shape=(28,)))\n",
    "poisson.add(Dense(10, activation='sigmoid'))\n",
    "poisson.add(Dense(1, activation='sigmoid'))\n",
    "poisson.compile(loss='poisson', optimizer='rmsprop', metrics=['accuracy'])\n",
    "poisson.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 211s 38us/step - loss: 0.5153 - acc: 0.7389 - val_loss: 0.5147 - val_acc: 0.7394\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 213s 39us/step - loss: 0.5152 - acc: 0.7392 - val_loss: 0.5150 - val_acc: 0.7390\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 202s 37us/step - loss: 0.5151 - acc: 0.7392 - val_loss: 0.5148 - val_acc: 0.7394\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 157s 29us/step - loss: 0.5149 - acc: 0.7394 - val_loss: 0.5141 - val_acc: 0.7398\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 147s 27us/step - loss: 0.5148 - acc: 0.7394 - val_loss: 0.5145 - val_acc: 0.7397\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 147s 27us/step - loss: 0.5146 - acc: 0.7396 - val_loss: 0.5142 - val_acc: 0.7396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a39e666d8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_lay.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 129s 23us/step - loss: 7.5479e-04 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 127s 23us/step - loss: -7.5802e-07 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 127s 23us/step - loss: -7.5802e-07 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 182s 33us/step - loss: -7.5802e-07 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 198s 36us/step - loss: -7.5802e-07 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 148s 27us/step - loss: -7.5802e-07 - acc: 0.5297 - val_loss: -7.5734e-07 - val_acc: 0.5301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a22ab9860>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kld.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".58\")\n",
    "accuracy_result.append(\".73\")\n",
    "label_result.append(\"3LyKLDAdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500000 samples, validate on 5500000 samples\n",
      "Epoch 1/6\n",
      "5500000/5500000 [==============================] - 157s 28us/step - loss: 0.8017 - acc: 0.7091 - val_loss: 0.7901 - val_acc: 0.7278\n",
      "Epoch 2/6\n",
      "5500000/5500000 [==============================] - 150s 27us/step - loss: 0.7885 - acc: 0.7299 - val_loss: 0.7872 - val_acc: 0.7319\n",
      "Epoch 3/6\n",
      "5500000/5500000 [==============================] - 189s 34us/step - loss: 0.7863 - acc: 0.7335 - val_loss: 0.7858 - val_acc: 0.7345\n",
      "Epoch 4/6\n",
      "5500000/5500000 [==============================] - 160s 29us/step - loss: 0.7853 - acc: 0.7351 - val_loss: 0.7853 - val_acc: 0.7353\n",
      "Epoch 5/6\n",
      "5500000/5500000 [==============================] - 157s 29us/step - loss: 0.7848 - acc: 0.7359 - val_loss: 0.7845 - val_acc: 0.7367\n",
      "Epoch 6/6\n",
      "5500000/5500000 [==============================] - 184s 33us/step - loss: 0.7846 - acc: 0.7363 - val_loss: 0.7846 - val_acc: 0.7366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a23473ac8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisson.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.append(\".78\")\n",
    "accuracy_result.append(\".73\")\n",
    "label_result.append(\"3LySigPoisRMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(activation_fn, loss_fn, optimizer_fn):\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(28, activation=activation_fn, input_shape=(28,)))\n",
    "    seq.add(Dense(10, activation=activation_fn))\n",
    "    seq.add(Dense(1, activation=activation_fn))\n",
    "    seq.compile(loss=loss_fn, optimizer=optimizer_fn, metrics=['accuracy'])\n",
    "    seq.summary()\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAIMCAYAAAB7di0zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhf8D6kTM8kco6iQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a33d68b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# display(loss_result.pop(7))\n",
    "# display(loss_result)\n",
    "accycolor = []\n",
    "#Generate Colors on red/green axis based on execution time\n",
    "for index in range(0,len(loss_result)):\n",
    "    \n",
    "    percent_red = float(loss_result[index])\n",
    "    percent_green = 1 - percent_red\n",
    "    red_10 = int(percent_red * 255)\n",
    "    green_10 = int(percent_green * 255)\n",
    "    red_16 = str(hex(red_10))[-2:].replace(\"x\", \"0\")\n",
    "    green_16 = str(hex(green_10))[-2:].replace(\"x\", \"0\")\n",
    "    accycolor.append(\"#\"+str(red_16)+str(green_16)+\"88\")\n",
    "\n",
    "fig, ax = plot.subplots(figsize=(11, 9))\n",
    "rects = ax.bar(label_result, accuracy_result, color=accycolor)\n",
    "random_chance = 0\n",
    "# plot.axhline(y=random_chance, color='r', linestyle='-')\n",
    "ax.set_ylim(ymin=0)\n",
    "# Indicate Times.\n",
    "labels = [\"%s\" % l for l in label_result]\n",
    "\n",
    "\n",
    "plot.plot()\n",
    "for rect, label in zip(rects, labels):\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, .1, label,ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS AN A AMZING DATASET\n",
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvYAAAIMCAYAAACe6OhfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nONhLkiRJA+BgL0mSJA3AP4a/F2yzr4MvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a34339898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losscolor = []\n",
    "#Generate Colors on red/green axis based on execution time\n",
    "for index in range(0,len(loss_result)):\n",
    "    \n",
    "    percent_red = float(loss_result[index])\n",
    "    percent_green = 1 - percent_red\n",
    "    red_10 = int(percent_red * 255)\n",
    "    green_10 = int(percent_green * 255)\n",
    "    red_16 = str(hex(red_10))[-2:].replace(\"x\", \"0\")\n",
    "    green_16 = str(hex(green_10))[-2:].replace(\"x\", \"0\")\n",
    "    losscolor.append(\"#\"+str(red_16)+str(green_16)+\"88\")\n",
    "\n",
    "fig, ax = plot.subplots(figsize=(11, 9))\n",
    "\n",
    "print('THIS IS AN A AMZING DATASET')\n",
    "print(type(left))\n",
    "rects = ax.barh(label_result, accuracy_result, color=accycolor)\n",
    "random_chance = 0\n",
    "plot.axhline(y=random_chance, color='r', linestyle='-')\n",
    "\n",
    "# Indicate Times.\n",
    "labels = [\"%s\" % l for l in label_result]\n",
    "\n",
    "\n",
    "plot.plot()\n",
    "# for rect, label in zip(rects, labels):\n",
    "#     ax.text(rect.get_x() + rect.get_width() / 2, .1, label,ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
